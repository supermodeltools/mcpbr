{% set name = "mcpbr" %}
{% set version = "0.6.0" %}

package:
  name: {{ name }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/m/{{ name }}/{{ name }}-{{ version }}.tar.gz
  sha256: PLACEHOLDER_SHA256

build:
  number: 0
  noarch: python
  script: {{ PYTHON }} -m pip install . --no-deps --no-build-isolation -vv
  entry_points:
    - mcpbr = mcpbr.cli:main

requirements:
  host:
    - python >=3.11
    - pip
    - hatchling
  run:
    - python >=3.11
    - anthropic >=0.40.0
    - mcp >=1.0.0
    - docker-py >=7.0.0
    - datasets >=2.14.0
    - click >=8.0.0
    - pydantic >=2.0.0
    - pydantic-settings >=2.0.0
    - pyyaml >=6.0.0
    - rich >=13.0.0
    - requests >=2.31.0
    - psutil >=5.9.0
    - paramiko >=3.4.0
    # Optional provider SDKs (not required for core Anthropic usage)
    # - openai >=1.0.0
    # - google-generativeai >=0.3.0

test:
  imports:
    - mcpbr
  commands:
    - mcpbr --help

about:
  home: https://github.com/greynewell/mcpbr
  license: MIT
  license_family: MIT
  license_file: LICENSE
  summary: Model Context Protocol Benchmark Runner - evaluate MCP servers against software engineering benchmarks
  description: |
    mcpbr is a benchmark runner for evaluating MCP (Model Context Protocol)
    servers against software engineering benchmarks like SWE-bench, HumanEval,
    and more.
  dev_url: https://github.com/greynewell/mcpbr
  doc_url: https://greynewell.github.io/mcpbr/
